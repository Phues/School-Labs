{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Cython in c:\\users\\benya\\anaconda3\\lib\\site-packages (3.0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\benya\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\benya\\anaconda3\\lib\\site-packages)\n",
      "DEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcbow\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32md:\\AIDS\\S2\\NLP\\TP2\\cbow.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\__init__.py:249\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124m            Failed to load PyTorch C extensions:\u001b[39m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124m                It appears that PyTorch has loaded the `torch/_C` folder\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124m                or by running Python from a different directory.\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m'''\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[43m_C\u001b[49m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    251\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "\u001b[1;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import cbow\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrixe\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable. (Failed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable. (request to http://localhost:8888/api/kernels?1709563270405 failed, reason: connect ECONNREFUSED 127.0.0.1:8888).)."
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('spooky_cleaned.csv')\n",
    "#Encode the labels using an encoding technique.\n",
    "le = LabelEncoder()\n",
    "df['author'] = le.fit_transform(df['author'])\n",
    "X = df['stemmed_text']\n",
    "y = df['author']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(vectors, words):\n",
    "    words = [word for word in words if word in vectors]\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(50)\n",
    "    return np.mean(vectors[words], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform column to string type\n",
    "df['stemmed_text'] = df['stemmed_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_w2v_cbow = [word_tokenize(text) for text in X_train]\n",
    "X_test_w2v_cbow = [word_tokenize(text) for text in X_test]\n",
    "\n",
    "vectors = Word2Vec(X_train_w2v_cbow, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "X_train_w2v_cbow = [vectorize(vectors.wv, sentence) for sentence in X_train_w2v_cbow]\n",
    "X_test_w2v_cbow = [vectorize(vectors.wv, sentence) for sentence in X_test_w2v_cbow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    tokenized_text = [token.text for token in nlp(text)]\n",
    "    vocab = set(tokenied_text)\n",
    "\n",
    "    word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "    idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "    data = []\n",
    "    for i in range(2, len(tokenized_text) - 2):\n",
    "        context = [\n",
    "            tokenized_text[i - 2],\n",
    "            tokenized_text[i - 1],\n",
    "            tokenized_text[i + 1],\n",
    "            tokenized_text[i + 2]\n",
    "        ]\n",
    "        target = tokenized_text[i]\n",
    "\n",
    "        context_idxs = [word_to_idx[w] for w in context]\n",
    "        target_idx = word_to_idx[target]\n",
    "        data.append((context_idxs, target_idx))\n",
    "    return data, word_to_idx, idx_to_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    EMBEDDING_SIZE = 50\n",
    "    data, word_to_idx, idx_to_word = create_dataset()\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    model = cbow.CBOW(len(word_to_idx), EMBEDDING_SIZE)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "    \n",
    "    context_data = torch.tensor([data[0] for data in data])\n",
    "    labels = torch.tensor([data[1] for data in data])\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(context_data, labels)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    for epoch in range(50):\n",
    "        for context, label in data_loader:\n",
    "            output = net(context)\n",
    "            loss = loss_func(output, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable. (Failed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable. (request to http://localhost:8888/api/kernels?1709562537480 failed, reason: connect ECONNREFUSED 127.0.0.1:8888).)."
     ]
    }
   ],
   "source": [
    "# convert x_train to text \n",
    "X = df['stemmed_text']\n",
    "y = df['author']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
