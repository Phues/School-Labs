{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1067156,"sourceType":"datasetVersion","datasetId":592212},{"sourceId":52294,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":43948}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom typing import Iterable, List\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset\nfrom timeit import default_timer as timer\nfrom torch.nn import Transformer\nfrom torch import Tensor\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport math\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:53:32.863823Z","iopub.execute_input":"2024-05-20T11:53:32.864169Z","iopub.status.idle":"2024-05-20T11:53:32.871152Z","shell.execute_reply.started":"2024-05-20T11:53:32.864140Z","shell.execute_reply":"2024-05-20T11:53:32.870115Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install -U portalocker\n!python -m spacy download en_core_web_sm\n!python -m spacy download fr_core_news_sm","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:53:32.872777Z","iopub.status.idle":"2024-05-20T11:54:21.919848Z","shell.execute_reply.started":"2024-05-20T11:53:32.873077Z","shell.execute_reply":"2024-05-20T11:54:21.918660Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting portalocker\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nInstalling collected packages: portalocker\nSuccessfully installed portalocker-2.8.2\nCollecting en-core-web-sm==3.7.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/conda/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.3)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\nCollecting fr-core-news-sm==3.7.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from fr-core-news-sm==3.7.0) (3.7.3)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.2)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.9.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.66.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.26.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2024.2.2)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.7)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.1.3)\nInstalling collected packages: fr-core-news-sm\nSuccessfully installed fr-core-news-sm-3.7.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('fr_core_news_sm')\n","output_type":"stream"}]},{"cell_type":"code","source":"#check if cuda is available\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:54:21.921875Z","iopub.execute_input":"2024-05-20T11:54:21.922196Z","iopub.status.idle":"2024-05-20T11:54:21.931144Z","shell.execute_reply.started":"2024-05-20T11:54:21.922164Z","shell.execute_reply":"2024-05-20T11:54:21.930218Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"seed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:16.539841Z","iopub.execute_input":"2024-05-20T11:56:16.540718Z","iopub.status.idle":"2024-05-20T11:56:16.548982Z","shell.execute_reply.started":"2024-05-20T11:56:16.540671Z","shell.execute_reply":"2024-05-20T11:56:16.548002Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"SRC_LANGUAGE = 'en'\nTGT_LANGUAGE = 'fr'","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:18.923796Z","iopub.execute_input":"2024-05-20T11:56:18.924443Z","iopub.status.idle":"2024-05-20T11:56:18.928719Z","shell.execute_reply.started":"2024-05-20T11:56:18.924409Z","shell.execute_reply":"2024-05-20T11:56:18.927655Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"token_transform = {}\nvocab_transform = {}\ntoken_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\ntoken_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='fr_core_news_sm')","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:19.103437Z","iopub.execute_input":"2024-05-20T11:56:19.103802Z","iopub.status.idle":"2024-05-20T11:56:22.477205Z","shell.execute_reply.started":"2024-05-20T11:56:19.103771Z","shell.execute_reply":"2024-05-20T11:56:22.476309Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"csv = pd.read_csv(\n    '/kaggle/input/language-translation-englishfrench/eng_-french.csv', \n    usecols=['English words/sentences', 'French words/sentences']\n)\ncsv.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:22.479056Z","iopub.execute_input":"2024-05-20T11:56:22.479382Z","iopub.status.idle":"2024-05-20T11:56:22.995852Z","shell.execute_reply.started":"2024-05-20T11:56:22.479353Z","shell.execute_reply":"2024-05-20T11:56:22.994724Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"  English words/sentences French words/sentences\n0                     Hi.                 Salut!\n1                    Run!                Cours !\n2                    Run!               Courez !\n3                    Who?                  Qui ?\n4                    Wow!             Ça alors !","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>English words/sentences</th>\n      <th>French words/sentences</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hi.</td>\n      <td>Salut!</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Run!</td>\n      <td>Cours !</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Run!</td>\n      <td>Courez !</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Who?</td>\n      <td>Qui ?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Wow!</td>\n      <td>Ça alors !</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_csv, test_csv = train_test_split(csv, test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:22.996907Z","iopub.execute_input":"2024-05-20T11:56:22.997179Z","iopub.status.idle":"2024-05-20T11:56:23.024204Z","shell.execute_reply.started":"2024-05-20T11:56:22.997156Z","shell.execute_reply":"2024-05-20T11:56:23.023477Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class TranslationDataset(Dataset):\n    def __init__(self, csv):\n        self.csv = csv\n        \n    def __len__(self):\n        return len(self.csv)\n    \n    def __getitem__(self, idx):\n        return(\n            self.csv['English words/sentences'].iloc[idx],\n            self.csv['French words/sentences'].iloc[idx]\n        )","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:23.026870Z","iopub.execute_input":"2024-05-20T11:56:23.027303Z","iopub.status.idle":"2024-05-20T11:56:23.033222Z","shell.execute_reply.started":"2024-05-20T11:56:23.027267Z","shell.execute_reply":"2024-05-20T11:56:23.032338Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_dataset = TranslationDataset(train_csv)\nvalid_dataset = TranslationDataset(test_csv)\niterator = iter(train_dataset)\nprint(next(iterator))","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:23.034293Z","iopub.execute_input":"2024-05-20T11:56:23.034571Z","iopub.status.idle":"2024-05-20T11:56:23.043364Z","shell.execute_reply.started":"2024-05-20T11:56:23.034546Z","shell.execute_reply":"2024-05-20T11:56:23.042358Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"('They kept him waiting outside for a long time.', 'Ils le firent poireauter dehors.')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- UNK_IDX: Represents unknown words.\n- PAD_IDX : Used for padding shorter sequences.\n- BOS_IDX : Denotes the beginning of a sequence.\n- EOS_IDX : Denotes the end of a sequence.","metadata":{}},{"cell_type":"code","source":"# Helper function to yield list of tokens.\ndef yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n    for data_sample in data_iter:\n        yield token_transform[language](data_sample[language_index[language]])\n# Define special symbols and indices.\nUNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n# Make sure the tokens are in order of their indices to properly insert them in vocab.\nspecial_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\nfor ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n    # Create torchtext's Vocab object.\n    vocab_transform[ln] = build_vocab_from_iterator(\n        yield_tokens(train_dataset, ln),\n        min_freq=1,\n        specials=special_symbols,\n        special_first=True,\n    )\n# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\nfor ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n    vocab_transform[ln].set_default_index(UNK_IDX)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:23.044347Z","iopub.execute_input":"2024-05-20T11:56:23.044622Z","iopub.status.idle":"2024-05-20T11:56:48.657488Z","shell.execute_reply.started":"2024-05-20T11:56:23.044598Z","shell.execute_reply":"2024-05-20T11:56:48.656567Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# helper function to club together sequential operations\ndef sequential_transforms(*transforms):\n    def func(txt_input):\n        for transform in transforms:\n            txt_input = transform(txt_input)\n        return txt_input\n    return func\n# function to add BOS/EOS and create tensor for input sequence indices\ndef tensor_transform(token_ids: List[int]):\n    return torch.cat((torch.tensor([BOS_IDX]),\n                      torch.tensor(token_ids),\n                      torch.tensor([EOS_IDX])))\n# `src` and `tgt` language text transforms to convert raw strings into tensors indices\ntext_transform = {}\nfor ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n    text_transform[ln] = sequential_transforms(token_transform[ln], # Tokenization\n                                               vocab_transform[ln], # Numericalization\n                                               tensor_transform) # Add BOS/EOS and create tensor\n# function to collate data samples into batch tensors\ndef collate_fn(batch):\n    src_batch, tgt_batch = [], []\n    for src_sample, tgt_sample in batch:\n        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n    return src_batch, tgt_batch","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:48.658572Z","iopub.execute_input":"2024-05-20T11:56:48.658868Z","iopub.status.idle":"2024-05-20T11:56:48.667722Z","shell.execute_reply.started":"2024-05-20T11:56:48.658842Z","shell.execute_reply":"2024-05-20T11:56:48.666732Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\nTGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\nEMB_SIZE = 192\nNHEAD = 6\nFFN_HID_DIM = 192\nBATCH_SIZE = 192\nNUM_ENCODER_LAYERS = 3\nNUM_DECODER_LAYERS = 3\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nNUM_EPOCHS = 50","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:48.668700Z","iopub.execute_input":"2024-05-20T11:56:48.668953Z","iopub.status.idle":"2024-05-20T11:56:48.682438Z","shell.execute_reply.started":"2024-05-20T11:56:48.668930Z","shell.execute_reply":"2024-05-20T11:56:48.681708Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def generate_square_subsequent_mask(sz):\n    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\ndef create_mask(src, tgt):\n    src_seq_len = src.shape[1]\n    tgt_seq_len = tgt.shape[1]\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n    src_padding_mask = (src == PAD_IDX)\n    tgt_padding_mask = (tgt == PAD_IDX)\n    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:48.683369Z","iopub.execute_input":"2024-05-20T11:56:48.683638Z","iopub.status.idle":"2024-05-20T11:56:48.693115Z","shell.execute_reply.started":"2024-05-20T11:56:48.683615Z","shell.execute_reply":"2024-05-20T11:56:48.692387Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        \"\"\"\n        :param max_len: Input length sequence.\n        :param d_model: Embedding dimension.\n        :param dropout: Dropout value (default=0.1)\n        \"\"\"\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n    def forward(self, x):\n        \"\"\"\n        Inputs of forward function\n        :param x: the sequence fed to the positional encoder model (required).\n        Shape:\n            x: [sequence length, batch size, embed dim]\n            output: [sequence length, batch size, embed dim]\n        \"\"\"\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:48.695288Z","iopub.execute_input":"2024-05-20T11:56:48.695581Z","iopub.status.idle":"2024-05-20T11:56:48.707265Z","shell.execute_reply.started":"2024-05-20T11:56:48.695555Z","shell.execute_reply":"2024-05-20T11:56:48.706489Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class TokenEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, emb_size):\n        super(TokenEmbedding, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_size)\n        self.emb_size = emb_size\n    def forward(self, tokens: Tensor):\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:56:48.708258Z","iopub.execute_input":"2024-05-20T11:56:48.708507Z","iopub.status.idle":"2024-05-20T11:56:48.717002Z","shell.execute_reply.started":"2024-05-20T11:56:48.708484Z","shell.execute_reply":"2024-05-20T11:56:48.716205Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class Seq2SeqTransformer(nn.Module):\n    def __init__(\n        self,\n        num_encoder_layers: int,\n        num_decoder_layers: int,\n        emb_size: int,\n        nhead: int,\n        src_vocab_size: int,\n        tgt_vocab_size: int,\n        dim_feedforward: int = 512,\n        dropout: float = 0.1\n    ):\n        super(Seq2SeqTransformer, self).__init__()\n        self.transformer = Transformer(\n            d_model=emb_size,\n            nhead=nhead,\n            num_encoder_layers=num_encoder_layers,\n            num_decoder_layers=num_decoder_layers,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n        self.positional_encoding = PositionalEncoding(\n            emb_size, dropout=dropout)\n    def forward(self,\n                src: Tensor,\n                trg: Tensor,\n                src_mask: Tensor,\n                tgt_mask: Tensor,\n                src_padding_mask: Tensor,\n                tgt_padding_mask: Tensor,\n                memory_key_padding_mask: Tensor):\n        src_emb = self.positional_encoding(self.src_tok_emb(src))\n        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n        return self.generator(outs)\n    def encode(self, src: Tensor, src_mask: Tensor):\n        return self.transformer.encoder(self.positional_encoding(\n                            self.src_tok_emb(src)), src_mask)\n    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n        return self.transformer.decoder(self.positional_encoding(\n                          self.tgt_tok_emb(tgt)), memory,\n                          tgt_mask)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:57:28.113226Z","iopub.execute_input":"2024-05-20T11:57:28.113865Z","iopub.status.idle":"2024-05-20T11:57:28.124882Z","shell.execute_reply.started":"2024-05-20T11:57:28.113831Z","shell.execute_reply":"2024-05-20T11:57:28.123849Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model = Seq2SeqTransformer(\n    NUM_ENCODER_LAYERS, \n    NUM_DECODER_LAYERS, \n    EMB_SIZE,\n    NHEAD, \n    SRC_VOCAB_SIZE, \n    TGT_VOCAB_SIZE, \n    FFN_HID_DIM\n).to(DEVICE)\n# Total parameters and trainable parameters.\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\")\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:57:28.880928Z","iopub.execute_input":"2024-05-20T11:57:28.881600Z","iopub.status.idle":"2024-05-20T11:57:29.269128Z","shell.execute_reply.started":"2024-05-20T11:57:28.881567Z","shell.execute_reply":"2024-05-20T11:57:29.268183Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"14,487,719 total parameters.\n14,487,719 training parameters.\nSeq2SeqTransformer(\n  (transformer): Transformer(\n    (encoder): TransformerEncoder(\n      (layers): ModuleList(\n        (0-2): 3 x TransformerEncoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n          )\n          (linear1): Linear(in_features=192, out_features=192, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=192, out_features=192, bias=True)\n          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): TransformerDecoder(\n      (layers): ModuleList(\n        (0-2): 3 x TransformerDecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n          )\n          (multihead_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n          )\n          (linear1): Linear(in_features=192, out_features=192, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=192, out_features=192, bias=True)\n          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (generator): Linear(in_features=192, out_features=25319, bias=True)\n  (src_tok_emb): TokenEmbedding(\n    (embedding): Embedding(15389, 192)\n  )\n  (tgt_tok_emb): TokenEmbedding(\n    (embedding): Embedding(25319, 192)\n  )\n  (positional_encoding): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:57:29.613564Z","iopub.execute_input":"2024-05-20T11:57:29.614291Z","iopub.status.idle":"2024-05-20T11:57:31.356043Z","shell.execute_reply.started":"2024-05-20T11:57:29.614258Z","shell.execute_reply":"2024-05-20T11:57:31.355255Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:57:34.235800Z","iopub.execute_input":"2024-05-20T11:57:34.236380Z","iopub.status.idle":"2024-05-20T11:57:34.241783Z","shell.execute_reply.started":"2024-05-20T11:57:34.236351Z","shell.execute_reply":"2024-05-20T11:57:34.240538Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"\ndef train_epoch(model, optimizer):\n    print('Training')\n    model.train()\n    losses = 0\n    for src, tgt in tqdm(train_dataloader, total=len(list(train_dataloader))):            \n        # print(\" \".join(vocab_transform[SRC_LANGUAGE].lookup_tokens(list(src[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n        # print(\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n        src = src.to(DEVICE)\n        tgt = tgt.to(DEVICE)\n                \n        tgt_input = tgt[:, :-1]\n        \n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n        logits = model(\n            src, \n            tgt_input, \n            src_mask, \n            tgt_mask,\n            src_padding_mask, \n            tgt_padding_mask, \n            src_padding_mask\n        )\n        optimizer.zero_grad()\n        tgt_out = tgt[:, 1:]\n        loss = loss_fn(logits.view(-1, TGT_VOCAB_SIZE), tgt_out.contiguous().view(-1))\n        loss.backward()\n        optimizer.step()\n        losses += loss.item()\n    return losses / len(list(train_dataloader))\nval_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\ndef evaluate(model):\n    print('Validating')\n    model.eval()\n    losses = 0\n    for src, tgt in tqdm(val_dataloader, total=len(list(val_dataloader))):\n        # print(\" \".join(vocab_transform[SRC_LANGUAGE].lookup_tokens(list(src[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n        # print(\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n        src = src.to(DEVICE)\n        tgt = tgt.to(DEVICE)\n        \n        tgt_input = tgt[:, :-1]\n        \n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n        \n        logits = model(\n            src, \n            tgt_input, \n            src_mask, \n            tgt_mask,\n            src_padding_mask, \n            tgt_padding_mask, \n            src_padding_mask\n        )\n        tgt_out = tgt[:, 1:]\n        loss = loss_fn(logits.view(-1, TGT_VOCAB_SIZE), tgt_out.contiguous().view(-1))\n        losses += loss.item()\n    return losses / len(list(val_dataloader))","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:57:35.934352Z","iopub.execute_input":"2024-05-20T11:57:35.934732Z","iopub.status.idle":"2024-05-20T11:57:35.947166Z","shell.execute_reply.started":"2024-05-20T11:57:35.934702Z","shell.execute_reply":"2024-05-20T11:57:35.946092Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train_loss_list, valid_loss_list = [], []\nfor epoch in range(1, NUM_EPOCHS+1):\n    start_time = timer()\n    train_loss = train_epoch(model, optimizer)\n    valid_loss = evaluate(model)\n    end_time = timer()\n    train_loss_list.append(train_loss)\n    valid_loss_list.append(valid_loss)\n    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {valid_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s \\n\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs('results', exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:18:59.056241Z","iopub.status.idle":"2024-05-19T17:18:59.056729Z","shell.execute_reply.started":"2024-05-19T17:18:59.056460Z","shell.execute_reply":"2024-05-19T17:18:59.056479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_plots(train_loss, valid_loss):\n    \"\"\"\n    Function to save the loss plots to disk.\n    \"\"\"\n    # Loss plots.\n    plt.figure(figsize=(10, 7))\n    plt.plot(\n        train_loss, color='blue', linestyle='-', \n        label='train loss'\n    )\n    plt.plot(\n        valid_loss, color='red', linestyle='-', \n        label='validataion loss'\n    )\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.savefig(os.path.join('results', 'loss.png'))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:18:59.057897Z","iopub.status.idle":"2024-05-19T17:18:59.058340Z","shell.execute_reply.started":"2024-05-19T17:18:59.058108Z","shell.execute_reply":"2024-05-19T17:18:59.058128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_plots(train_loss_list, valid_loss_list)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:18:59.059866Z","iopub.status.idle":"2024-05-19T17:18:59.060206Z","shell.execute_reply.started":"2024-05-19T17:18:59.060032Z","shell.execute_reply":"2024-05-19T17:18:59.060046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, 'model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:18:59.061196Z","iopub.status.idle":"2024-05-19T17:18:59.061559Z","shell.execute_reply.started":"2024-05-19T17:18:59.061402Z","shell.execute_reply":"2024-05-19T17:18:59.061415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torch.load('model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:57:40.784560Z","iopub.execute_input":"2024-05-20T11:57:40.785397Z","iopub.status.idle":"2024-05-20T11:57:40.852117Z","shell.execute_reply.started":"2024-05-20T11:57:40.785356Z","shell.execute_reply":"2024-05-20T11:57:40.851025Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Helper function to generate output sequence using greedy algorithm.\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    src = src.to(DEVICE)\n    src_mask = src_mask.to(DEVICE)\n    memory = model.encode(src, src_mask)\n    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n    for i in range(max_len-1):\n        memory = memory.to(DEVICE)\n        if i == 0:\n            ys = ys.transpose(1, 0)\n        tgt_mask = (generate_square_subsequent_mask(ys.size(1))\n                    .type(torch.bool)).to(DEVICE)\n        out = model.decode(ys, memory, tgt_mask)\n        out = out\n        prob = model.generator(out[:, -1])\n        _, next_word = torch.max(prob, dim=1)\n        next_word = next_word.item()\n        ys = torch.cat([ys,\n                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n        if next_word == EOS_IDX:\n            break\n    return ys\n# Translation function. \ndef translate(model: torch.nn.Module, src_sentence: str):\n    model.eval()\n    src = text_transform[SRC_LANGUAGE](src_sentence).view(1, -1)\n    num_tokens = src.shape[1]\n    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n    tgt_tokens = greedy_decode(\n        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:57:43.428188Z","iopub.execute_input":"2024-05-20T11:57:43.428956Z","iopub.status.idle":"2024-05-20T11:57:43.439998Z","shell.execute_reply.started":"2024-05-20T11:57:43.428922Z","shell.execute_reply":"2024-05-20T11:57:43.439120Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# SRC, GT pairs from the validation set.\ninfer_sentences = [\n    [\"Take a seat.\", \"Prends place !\"],\n    [\"I'm not scared to die\", \"Je ne crains pas de mourir.\"],\n    [\"You'd better make sure that it is true.\", \"Tu ferais bien de t'assurer que c'est vrai.\"],\n    [\"The clock has stopped.\", \"L'horloge s'est arrêtée.\"],\n    [\"Take any two cards you like.\", \"Prends deux cartes de ton choix.\"]\n]\nfor sentence in infer_sentences:\n    print(f\"SRC: {sentence[0]}\")\n    print(f\"GT: {sentence[1]}\")\n    print(f\"PRED: {translate(model, sentence[0])}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:57:44.253313Z","iopub.execute_input":"2024-05-20T11:57:44.254000Z","iopub.status.idle":"2024-05-20T11:57:45.054245Z","shell.execute_reply.started":"2024-05-20T11:57:44.253965Z","shell.execute_reply":"2024-05-20T11:57:45.053336Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"SRC: Take a seat.\nGT: Prends place !\nPRED:  Prenez une place ! \n\nSRC: I'm not scared to die\nGT: Je ne crains pas de mourir.\nPRED:  Je ne suis pas effrayé à mourir . \n\nSRC: You'd better make sure that it is true.\nGT: Tu ferais bien de t'assurer que c'est vrai.\nPRED:  Vous feriez mieux de vous assurer que c' est vrai . \n\nSRC: The clock has stopped.\nGT: L'horloge s'est arrêtée.\nPRED:  Le horloge s' est arrêté . \n\nSRC: Take any two cards you like.\nGT: Prends deux cartes de ton choix.\nPRED:  Prenez les cartes de cartes , comme vous tous les deux . \n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}